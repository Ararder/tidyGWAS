---
title: "tidyGWAS"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tidyGWAS}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Installation

tidyGWAS is not yet on CRAN. To install, you need to use
`devtools::install_github()` or `remotes::install_github()`

```{r, eval = FALSE}
devtools::install_github("ararder/tidyGWAS")
remotes::install_github("ararder/tidyGWAS")
```

Secondly, you will need to download the reference data. tidyGWAS uses a
slightly edited version of dbSNP v155, that is available
[here](https://zenodo.org/records/12805493).

```{bash, eval = FALSE}
wget https://zenodo.org/records/12805493/files/dbSNP155_v0.9.tar
tar -xvf dbSNP155_v0.9.tar

```


# Quick start

This is what a typical call to `tidyGWAS()` will look like. Continue reading for a more detailed explanation of each argument.


```{r, eval=FALSE}
# we setup a directory where we will store all summary statistics cleaned by tidyGWAS
gwas_folder <- tempfile()
# create a new folder inside, based on the name of the GWAS
outdir <- paste0(gwas_folder, "gwasName")

cleaned <- tidyGWAS(
  tbl = "/filepath/to/sumstats", 
  # we read in a tab-separated file, so provide tab as delimiter
  delim = "\t",
  dbsnp_path = dsnp_path,
  # 
  column_names = list(
    "CHR" = "CHROM",
    "POS" = "BP",
    "EffectAllele" = "A1",
    "B" = "Effect"
  ),
  # Number of samples are missing, so manually impute
  CaseN = 54000,
  ControlN = 73000,
  logfile=TRUE,
  output_dir = outdir
  )
```


# Walk-through tutorial
Here a longer tutorial starts, walking through the different arguments of `tidyGWAS()`.

To run tidyGWAS on your summary statistics of choice, provide
a in-memory `dplyr::tibble()` or a filepath. In this example, a filepath
to a gzipped tsv file is passed. tidyGWAS uses
`arrow::read_delim_arrow()` to read in files. Since we are using a tsv
file, we provide the argument `delim` with `"\t"`.

```{r}
library(tidyGWAS)
# we use the dummy version of dbSNP that comes with the package
dbsnp_path <- system.file("extdata/dbSNP155", package = "tidyGWAS")
# a dummy sumstats with 100 000 rows
# gwas <- system.file("extdata", package="tidyGWAS") |> paste0("/sumstats.tsv.gz")
gwas <- system.file("extdata/sumstats.tsv.gz", package = "tidyGWAS")
# store the results in a temporary directory
out <- tempfile()

tidyGWAS(
  tbl = gwas,
  delim = "\t",
  dbsnp_path = dbsnp_path,
  output_dir = out
)
```

That's a lot of files!

1.  Removed rows are stored in `pipeline/removed_*`
2.  `metadata.yaml` contains metadata about the execution
3.  `raw` contains the summary statistics ***before*** any munging was
    done. Useful to reproduce, or to identify why rows were removed.
4.  `tidyGWAS_hivestyle` contains the cleaned summary statistics, in
    something called a [hivestyle
    partition](https://arrow.apache.org/docs/r/articles/dataset.html),
    by default. The motivation for this is detailed further
    [down](#hivestyle-partitioning) in the vignette. If you just want a
    standard csv file, use `output_format="csv"`

```{r}
fs::dir_tree(out)
```

### Setting column names

Almost certainly, the summary statistics you want to clean will not have
the same column name that the example file had.

```{r}
sumstats <- read.table(gwas, header=T) |> dplyr::tibble()
head(sumstats)
```

```{r}
colnames(sumstats)
```

`tidyGWAS()` will not guess which columns are which, and therefore
requires either the correct column names to exist before cleaning, or a
mapping between tidyGWAS column names and the column names in the input
file. This is done using the `column_names` argument, which takes a
named list, where the names are the tidyGWAS columns and the values the
corresponding column in the input file.

```{r, eval = FALSE}
# what if the names were all wrong?
sumstats_with_wrong_names <- dplyr::rename(
  sumstats, 
  CHROM = CHR, 
  BP = POS,
  ID = RSID,
  A1 = EffectAllele, 
  A2 = OtherAllele,
  EFFECT = B
)

tidyGWAS(
  tbl = sumstats_with_wrong_names, 
  dbsnp_path = dbsnp_path,
  column_names = list(
    CHR = "CHROM",
    POS = "BP",
    RSID = "ID",
    EffectAllele = "A1",
    OtherAllele = "A2",
    B = "EFFECT"
  )
  )

```

### The tidyGWAS nomenclature

tidyGWAS uses the following column names:

-   CHR

-   POS

-   RSID

-   EffectAllele

-   OtherAllele

-   EAF

-   B

-   SE

-   P

-   CaseN

-   ControlN

-   N

-   INFO

-   Z

**Note**:

If your RSID column is in the format CHR:BP:A1:A2, you can still pass it
as an RSID column.

### Inputting sample size columns

Often, the sample size column is missing from the summary statistics,
and you provide it manually. tidyGWAS has three arguments that can be
used to manually set the sample size, if it's missing from the original
file:

-   `CaseN`

-   `ControlN`

-   `N`

```{r, eval = FALSE}

cleaned <- tidyGWAS(
  tbl = sumstats, 
  dbsnp_path = dbsnp_path,
  # CaseN, ControlN and N can all be used to set sample size
  CaseN = 400,
  ControlN = 800,
  N = 1200
  )


```

### Reading files directly from disk

If you pass a filepath to tidyGWAS, it will attempt to read in the file
with `arrow::read_delim_arrow()`

The default delimiter is white space, so to read in comma-separated
files or tab-separated files, you can provide the `delim` argument to
`arrow::read_delim_arrow()` through `...`

```{r, eval=FALSE}

cleaned <- tidyGWAS(
  tbl = "filepath/to/gwas/ondisk.csv",
  # here we specify the delimiter to for csv files
  delim = ",",
  dbsnp_path = dbsnp_path,
)
```

There's a lot of different field delimiters used in the wild, and
sometimes you can struggle with inputting the correct delimiter. In such
cases, it's often much more convenient to use the effective
`data.table::fread()` or `readr::read_table()` to first read in the
summary statistics into memory before passing it to tidyGWAS.

```{r, eval=FALSE}
# use readr
sumstats <- readr::read_table("path/to/sumstats.gz.vcf")
# or data.table
sumstats <- data.table::fread("path/to/sumstats.gz.vcf")

output <- "my_gwas_dir"

cleaned <- tidyGWAS(
  tbl = sumstats,
  dbsnp_path = dbsnp_path,
  outdir = output
)

```

# Hivestyle-partitioning {#hivestyle-partitioning}

The default output format is a hivestyle format using .parquet files.
See for example
[here](https://arrow.apache.org/docs/r/articles/dataset.html) for some
motivation for this. In essence, this format will significantly speed up
other downstream applications such as meta-analysis, LD querying and
other analyses.

```{r, eval=FALSE}
dir_tree(fs::path(gwas_folder,"scz_pgc3", "tidyGWAS_hivestyle"), recurse =2)

# This is the equivalent of readr::read_tsv() or data.table::fread() for 
# the hivestyle format.
```

# The output format

The partitioned .parquet files can easily be read into memory using
`arrow::open_dataset() |> collect()`.

`tidyGWAS()` will add a set of columns, some depending on what existed
in the input columns.

### Variant identity

1.  `CHR` Chromosome. The same across both builds

2.  `POS_38`, `POS_37` genomic position on GRCh38 and GRCh37

3.  `RSID` variant ID from dbSNP

4.  `REF_37`, `REF_38` is the reference genome allele on GRCh38 and
    GRCh37

5.  `multi_allelic` is a TRUE/FALSE column that flag rows that were
    multi-allelic IN the summary statistics NOT whether there are
    multiple alleles in dbSNP. (TRUE corresponds to multi allelic).

6.  `rowid` maps each row back to the inputted summary statistics the
    file in `raw`, so that any row can be mapped back to it's original
    values.

### Statistics columns

statistics columns such as `B, P, SE, Z` will be added if missing and if
it is possible to impute them using existing columns.



# Parallel computation

tidyGWAS automatically detects the number of cores. In some cases, for example when
running tidyGWAS in a HPC cluster, you might need to manually set the
number of cores, which can be done using the `OMP_NUM_THREADS` variable.
This should not be a larger number of cores than what you have requested
in your HPC job (in the example below, the "--cpus-per-task" flag)

```{bash, eval=FALSE}

#SBATCH --mem=60gb
#SBATCH --time=24:0:00
#SBATCH --cpus-per-task 8
export OMP_NUM_THREADS=8

outdir=$(pwd)
gwas=$outdir/my_gwas.tsv.gz
dbsnp_files="dbSNP155"
Rscript -e "tidyGWAS(commandArgs(trailingOnly = TRUE)[1],  dbsnp_path = commandArgs(trailingOnly = TRUE)[2],outdir = commandArgs(trailingOnly = TRUE)[3], logfile=TRUE)" $gwas $dbsnp_files $outdir

```

# Computational cost and memory usage

Memory use and time scales with the size of the summary statistics. From
running tidyGWAS, here's an estimation:

The memory usage will likely be the main constraint with increasing size
of summary statistics.

1.  20 million rows \~20gb
2.  40 million rows \~40gb
3.  60 million rows \~75gb


